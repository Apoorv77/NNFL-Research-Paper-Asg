{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploring Tokenization.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AyUZwYYTsAW"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV9uVKNjWkks"
      },
      "source": [
        "Updating fastai library on collab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlW7MZSYWMx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddf7b7d-ca9f-4432-d3eb-b8c4313752f3"
      },
      "source": [
        "! [ -e /content ] && pip install -Uqq fastai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 40 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 51 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 61 kB 31.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 81 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 92 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 102 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 122 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 133 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 143 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 153 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 163 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 174 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 184 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 189 kB 27.5 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51 kB 38.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIPRiJodVkfQ"
      },
      "source": [
        "Here, We are downloading the IMDB Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "2TL3VYBRTpRv",
        "outputId": "7df6e37e-fb78-462e-dc74-7ebb064acaa8"
      },
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:02<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgMlh7OgTrD6"
      },
      "source": [
        "To see what was downloaded: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n8eiy-rXUta",
        "outputId": "f2fe00e1-54ee-4335-86d1-193c941b7cb2"
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#7) [Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/README'),Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/imdb.vocab')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5sTzieGZFml"
      },
      "source": [
        "Getting all the text files from the three folders : Unsupervised, Test, And Training Folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vr-fZhnYtd_",
        "outputId": "3f40112f-f837-48b4-d410-cc0bfbf49554"
      },
      "source": [
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n",
        "\n",
        "print(files[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Path('/root/.fastai/data/imdb/unsup/38932_0.txt'), Path('/root/.fastai/data/imdb/unsup/896_0.txt'), Path('/root/.fastai/data/imdb/unsup/16152_0.txt'), Path('/root/.fastai/data/imdb/unsup/41284_0.txt'), Path('/root/.fastai/data/imdb/unsup/8306_0.txt')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7DzwG3pZnaE"
      },
      "source": [
        "An Example text: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8qQhXZX-Zb6a",
        "outputId": "7c86ea94-c84e-4762-ada5-20ef6bf507a9"
      },
      "source": [
        "txt = files[0].open().read(); txt[:78]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I don't think I've ever seen a show suck so hard! She might be a single mother\""
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GneVlPaSXxCf"
      },
      "source": [
        "Here, We do the tokenization on the first file. spacy doesnt just separate based on spaces, it also does things like splitting dont into do and n't as these are two separate words.\n",
        "\n",
        "Reference for spacey: [spaCy](https://spacy.io/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE3Hzo91lKcw",
        "outputId": "699aca10-4c07-4ce0-c03e-04c8fec028ff"
      },
      "source": [
        "spacy = WordTokenizer()\n",
        "tokWord = first(spacy([txt]))\n",
        "print(coll_repr(tokWord, 30))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pba9_-ManLfZ"
      },
      "source": [
        "It is possible to go even furthur in tokenization using fastai's wrapper on tokenization. It adds other features on top of spaCy like removing unnecessary spaces, adding tokens for things like beginning of stream, upperscale sletters, and much more.\n",
        "\n",
        "Here is a brief summary of all the things done:\n",
        "\n",
        "- `fix_html`:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n",
        "- `replace_rep`:: Replaces any character repeated three times or more with a special token for repetition (`xxrep`), the number of times it's repeated, then the character\n",
        "- `replace_wrep`:: Replaces any word repeated three times or more with a special token for word repetition (`xxwrep`), the number of times it's repeated, then the word\n",
        "- `spec_add_spaces`:: Adds spaces around / and #\n",
        "- `rm_useless_spaces`:: Removes all repetitions of the space character\n",
        "- `replace_all_caps`:: Lowercases a word written in all caps and adds a special token for all caps (`xxup`) in front of it\n",
        "- `replace_maj`:: Lowercases a capitalized word and adds a special token for capitalized (`xxmaj`) in front of it\n",
        "- `lowercase`:: Lowercases all text and adds a special token at the beginning (`xxbos`) and/or the end (`xxeos`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg0oyl0KoXDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2a10a4-5264-4104-c6b5-7d0b69bf4372"
      },
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#164) ['xxbos','i','do',\"n't\",'think','xxmaj','i',\"'ve\",'ever','seen','a','show','suck','so','hard','!','xxmaj','she','might','be','a','single','mother',',','but','a','mother','with','a','lot','of'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL2-we5ZpfxG"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "### Subword Level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5nI7pNRpv3v"
      },
      "source": [
        "Subword tokenization is also useful when there are no spaces differentiating words like in chinese.\n",
        "\n",
        "This is useful in order to reduce the size of our vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "*   We use the first 2000 reviews for our corpus\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lvWAo11syyA"
      },
      "source": [
        "corpus2000 = L(o.open().read() for o in files[:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeDJPpmxtyI2"
      },
      "source": [
        "Then we define a subword function that takes in a vocabulary size.\n",
        "\n",
        "This tokenizer first goes through the entire corpus and creates a frequency count and selects the top `sz` number of words and makes that into a vocabulary.\n",
        "\n",
        "After that it tokenizes the corpus and returns it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvW0aoBetytk"
      },
      "source": [
        "def subword(sz):\n",
        "    sp = SubwordTokenizer(vocab_sz=sz)\n",
        "    sp.setup(txts)\n",
        "    return ' '.join(first(sp([txt]))[:40])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce2yT6DwAio"
      },
      "source": [
        "Trying out with a vocabulary size of 2000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQn7Cr_6wF2Y"
      },
      "source": [
        "subword(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3MlGx2hwcQ0"
      },
      "source": [
        "a larger vocab means fewer tokens per sentence, which means faster training , less memory, and less states for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn. \n",
        "\n",
        "_ indicates space in the token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fzkLseQCqLd"
      },
      "source": [
        "## Numericalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQnQq-ThCscl"
      },
      "source": [
        "in numericalization, we map the tokes to integers based on their index in the vocabulary. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcjQZXVlvaXv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c248841-c270-48d6-f8a5-e5053644b8a3"
      },
      "source": [
        "toks200 = corpus2000[:2000].map(tkn)\n",
        "num = Numericalize(min_freq=3)\n",
        "num.setup(toks200)\n",
        "coll_repr(num.vocab,20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#10592) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','of','to','is','it','in','i'...]\""
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLOMd7sNGzej",
        "outputId": "3a32e01a-9e07-4191-fcf6-30cdad09d1b1"
      },
      "source": [
        "nums = num(toks)[:20]; nums\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorText([   0,   60,   34,  122,    0,  168,  137,  125,   13,  150, 1937,   53,  290,   55,    0,  266,   44,   13,  676,  379])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JAJQecd8Hncn",
        "outputId": "03eea107-acd2-4932-ec0c-0d49c79b27f8"
      },
      "source": [
        "' '.join(num.vocab[o] for o in nums)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"xxunk do n't think xxunk 've ever seen a show suck so hard ! xxunk might be a single mother\""
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh92PUVpJGhA"
      },
      "source": [
        "def makeDataset(dt):\n",
        "  x = []\n",
        "  y = []\n",
        "  tokennum = []\n",
        "  for text in dt:\n",
        "    tokennum.append(int(text))\n",
        "  \n",
        "  for i in range(len(tokennum) - 4):\n",
        "      x.append(tokennum[i:i+4]);\n",
        "      y.append(tokennum[i+4]);\n",
        "  return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HJQYNnpJKk5"
      },
      "source": [
        "four,one = makeDataset(nums)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRckTZZrNWHQ",
        "outputId": "8254cd1f-6c55-4dbb-b20f-286add5893ed"
      },
      "source": [
        "print(four)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 65, 39, 143], [65, 39, 143, 0], [39, 143, 0, 201], [143, 0, 201, 130], [0, 201, 130, 116], [201, 130, 116, 12], [130, 116, 12, 136], [116, 12, 136, 0], [12, 136, 0, 47], [136, 0, 47, 322], [0, 47, 322, 48], [47, 322, 48, 0], [322, 48, 0, 291], [48, 0, 291, 41], [0, 291, 41, 12], [291, 41, 12, 616]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNlKY8jHNcjJ",
        "outputId": "2b0f24a2-ed00-41cb-c7c3-eabf1e2070db"
      },
      "source": [
        "print(one)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 201, 130, 116, 12, 136, 0, 47, 322, 48, 0, 291, 41, 12, 616, 553]\n"
          ]
        }
      ]
    }
  ]
}